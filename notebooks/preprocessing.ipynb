{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f601c49-ecfe-4c1d-9110-dd370dcfb5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = os.path.join(os.path.dirname(os.getcwd()), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa6a20-3293-4f39-8e8e-473c6174430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main dataset\n",
    "full_data_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"original\", \"card_transdata.csv\")\n",
    "df = pd.read_csv(full_data_path)\n",
    "print(\"Dataset loaded.\")\n",
    "\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3a83f-1cb1-4577-a5ac-d8759649e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of samples: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d200c-b676-4521-9d2d-a7ff06ffd135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7985e7-4c2d-4920-a680-a56783be58e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x='fraud', hue='fraud', palette='Set2', legend=False)\n",
    "plt.title(\"Distribution of Fraud Labels\")\n",
    "plt.xlabel(\"Fraud (0 = Normal, 1 = Fraud)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d122d1-b357-4f32-bc9c-e1e032378003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into stratified train and test sets\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=df[\"fraud\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae9099-2667-440c-b014-956b3683795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the split datasets\n",
    "save_splits = True \n",
    "output_dir = os.path.join(os.path.dirname(os.getcwd()), \"data\")\n",
    "if save_splits:\n",
    "    train_path = os.path.join(output_dir, \"training\", \"card_transdata_part1.csv\")\n",
    "    test_path = os.path.join(output_dir, \"testing\", \"card_transdata_part2.csv\")\n",
    "\n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "\n",
    "    print(\"\\nStratified split completed and files saved.\")\n",
    "    print(f\"Train set path: {train_path}\")\n",
    "    print(f\"Test set path:  {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d3ea47-bc8a-4ec3-b930-74a6e7accfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display class distribution of the 'fraud' column\n",
    "print(\"\\n--- FRAUD Class Distribution ---\")\n",
    "\n",
    "print(\"\\n[Train Set - Part 1]\")\n",
    "print(train_df[\"fraud\"].value_counts(normalize=True))\n",
    "print(train_df[\"fraud\"].value_counts())\n",
    "\n",
    "print(\"\\n[Test Set - Part 2]\")\n",
    "print(test_df[\"fraud\"].value_counts(normalize=True))\n",
    "print(test_df[\"fraud\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19384bcb-6bcc-442c-bd84-13108c04de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train_df.drop(columns=['fraud'])\n",
    "y_train = train_df['fraud']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Class distribution before SMOTE:\", Counter(y_train))\n",
    "print(\"Class distribution after SMOTE:\", Counter(y_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29618ace-3a79-464b-813b-607215b7db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "counter_res = Counter(y_resampled)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(counter_res.keys(), counter_res.values(), color=['teal', 'coral'])\n",
    "plt.title('Class Distribution After SMOTE (Train Set Only)')\n",
    "plt.xlabel('Fraud Label (0 = Normal, 1 = Fraud)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2465bd9a-ce2f-43b9-a24f-fb9f58ecbdf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_smote = pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "df_smote['fraud'] = y_resampled\n",
    "\n",
    "df_smote.to_csv(\"../data/training/train_data_smote.csv\", index=False)\n",
    "\n",
    "print(\" SMOTE applied data was saved to the file 'train_data_smote.csv'.\")\n",
    "print(df_smote['fraud'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db454ab4-7575-43a9-ba49-4c1c8ed7c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_path = \"../data/training/train_data_smote.csv\"\n",
    "df = pd.read_csv(full_data_path)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adeda54-6fc3-4308-ba6a-e691ab899a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of samples: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2c95b-c01d-41be-97d0-4ad093b87476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_df = pd.read_csv(\"../data/training/train_data_smote.csv\")\n",
    "test_df = pd.read_csv(\"../data/testing/card_transdata_part2.csv\")\n",
    "\n",
    "# List of features to normalize\n",
    "feature_columns = [\n",
    "    'distance_from_home',\n",
    "    'distance_from_last_transaction',\n",
    "    'ratio_to_median_purchase_price',\n",
    "    'repeat_retailer',\n",
    "    'used_chip',\n",
    "    'used_pin_number',\n",
    "    'online_order'\n",
    "]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit on train data and transform both train and test sets\n",
    "train_df[feature_columns] = scaler.fit_transform(train_df[feature_columns])\n",
    "test_df[feature_columns] = scaler.transform(test_df[feature_columns])\n",
    "\n",
    "train_df.to_csv(\"../data/training/train_data_smote_normalized.csv\", index=False)\n",
    "test_df.to_csv(\"../data/testing/card_transdata_part2_normalized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8846503-9d48-4c46-8e11-fc501598288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "full_data_path = \"../data/testing/card_transdata_part2_normalized.csv\"\n",
    "df = pd.read_csv(full_data_path)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28685249-3c64-4106-97c1-cdcb8cf46aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "full_data_path = \"../data/training/train_data_smote_normalized.csv\"\n",
    "df = pd.read_csv(full_data_path)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fedb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copyfile(\n",
    "    os.path.join(DATA_DIR, \"training\", \"train_data_smote_normalized.csv\"),\n",
    "    os.path.join(DATA_DIR, \"training\", \"train_normal_1.csv\")\n",
    ")\n",
    "shutil.copyfile(\n",
    "    os.path.join(DATA_DIR, \"testing\", \"card_transdata_part2_normalized.csv\"),\n",
    "    os.path.join(DATA_DIR, \"testing\", \"test_normal_1.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430adafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_df = pd.read_csv(\"../data/training/train_data_smote.csv\")\n",
    "test_df = pd.read_csv(\"../data/testing/card_transdata_part2.csv\")\n",
    "\n",
    "# List of features to normalize\n",
    "feature_columns = [\n",
    "    'distance_from_home',\n",
    "    'distance_from_last_transaction',\n",
    "    'ratio_to_median_purchase_price',\n",
    "    'repeat_retailer',\n",
    "    'used_chip',\n",
    "    'used_pin_number',\n",
    "    'online_order'\n",
    "]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on train data and transform both train and test sets\n",
    "train_df[feature_columns] = scaler.fit_transform(train_df[feature_columns])\n",
    "test_df[feature_columns] = scaler.transform(test_df[feature_columns])\n",
    "\n",
    "train_df.to_csv(\"../data/training/train_normal_2.csv\", index=False)\n",
    "test_df.to_csv(\"../data/testing/test_normal_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f1d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "train_df = pd.read_csv(\"../data/training/train_data_smote.csv\")\n",
    "test_df = pd.read_csv(\"../data/testing/card_transdata_part2.csv\")\n",
    "\n",
    "# List of features to normalize\n",
    "feature_columns = [\n",
    "    'distance_from_home',\n",
    "    'distance_from_last_transaction',\n",
    "    'ratio_to_median_purchase_price',\n",
    "    'repeat_retailer',\n",
    "    'used_chip',\n",
    "    'used_pin_number',\n",
    "    'online_order'\n",
    "]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit on train data and transform both train and test sets\n",
    "train_df[feature_columns] = scaler.fit_transform(train_df[feature_columns])\n",
    "test_df[feature_columns] = scaler.transform(test_df[feature_columns])\n",
    "\n",
    "train_df.to_csv(\"../data/training/train_normal_3.csv\", index=False)\n",
    "test_df.to_csv(\"../data/testing/test_normal_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 5000\n",
    "\n",
    "def sample_df(df):\n",
    "    return df.sample(n=min(SAMPLE_SIZE, len(df)), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def compare_normalizations(original_df, norm_dfs, norm_names):\n",
    "    results = []\n",
    "\n",
    "    def compute_metrics(df, name):\n",
    "        desc = df.describe().T\n",
    "        desc['skew'] = df.apply(skew)\n",
    "        desc['kurtosis'] = df.apply(kurtosis)\n",
    "        desc['normalization'] = name\n",
    "        results.append(desc)\n",
    "\n",
    "        # Plot boxplots\n",
    "        df.boxplot(figsize=(10, 5))\n",
    "        plt.title(f'Boxplot for {name}')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot histograms (first few features for brevity)\n",
    "        df.iloc[:, :5].hist(figsize=(15, 8), bins=30)\n",
    "        plt.suptitle(f'Histograms for {name}')\n",
    "        plt.show()\n",
    "\n",
    "        # PCA scatter plot\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced = pca.fit_transform(df)\n",
    "        plt.scatter(reduced[:, 0], reduced[:, 1], alpha=0.5)\n",
    "        plt.title(f'PCA Scatter plot for {name}')\n",
    "        plt.xlabel('PC1')\n",
    "        plt.ylabel('PC2')\n",
    "        plt.show()\n",
    "\n",
    "        # Pairwise distance stats\n",
    "        dists = pairwise_distances(df)\n",
    "        print(f\"{name} - Mean pairwise distance: {dists.mean():.4f}, Std: {dists.std():.4f}\")\n",
    "\n",
    "    # Compute metrics for the original data\n",
    "    compute_metrics(original_df, 'Original')\n",
    "\n",
    "    # Compute metrics for each normalization\n",
    "    for norm_df, norm_name in zip(norm_dfs, norm_names):\n",
    "        compute_metrics(norm_df, norm_name)\n",
    "\n",
    "    # Combine and display metrics\n",
    "    summary = pd.concat(results)\n",
    "    print(summary)\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Use sample_df() to sample each dataframe for faster visualization\n",
    "train_original_df = sample_df(pd.read_csv(\"../data/training/train_data_smote.csv\"))\n",
    "train_norm1_df = sample_df(pd.read_csv(\"../data/training/train_normal_1.csv\"))\n",
    "train_norm2_df = sample_df(pd.read_csv(\"../data/training/train_normal_2.csv\"))\n",
    "train_norm3_df = sample_df(pd.read_csv(\"../data/training/train_normal_3.csv\"))\n",
    "\n",
    "test_original_df = sample_df(pd.read_csv(\"../data/testing/card_transdata_part2.csv\"))\n",
    "test_norm1_df = sample_df(pd.read_csv(\"../data/testing/test_normal_1.csv\"))\n",
    "test_norm2_df = sample_df(pd.read_csv(\"../data/testing/test_normal_2.csv\"))\n",
    "test_norm3_df = sample_df(pd.read_csv(\"../data/testing/test_normal_3.csv\"))\n",
    "\n",
    "# Drop 'fraud' column from all dataframes before comparison\n",
    "train_original_df = train_original_df.drop(columns=['fraud'])\n",
    "train_norm1_df = train_norm1_df.drop(columns=['fraud'])\n",
    "train_norm2_df = train_norm2_df.drop(columns=['fraud'])\n",
    "train_norm3_df = train_norm3_df.drop(columns=['fraud'])\n",
    "\n",
    "test_original_df = test_original_df.drop(columns=['fraud'])\n",
    "test_norm1_df = test_norm1_df.drop(columns=['fraud'])\n",
    "test_norm2_df = test_norm2_df.drop(columns=['fraud'])\n",
    "test_norm3_df = test_norm3_df.drop(columns=['fraud'])\n",
    "\n",
    "# Compare normalizations on training data\n",
    "print(\"Comparing normalizations on training data...\")\n",
    "train_results = compare_normalizations(\n",
    "    train_original_df,\n",
    "    [train_norm1_df, train_norm2_df, train_norm3_df],\n",
    "    ['MinMax', 'Standard', 'Robust']\n",
    ")\n",
    "\n",
    "# Compare normalizations on testing data\n",
    "print(\"Comparing normalizations on testing data...\")\n",
    "test_results = compare_normalizations(\n",
    "    test_original_df,\n",
    "    [test_norm1_df, test_norm2_df, test_norm3_df],\n",
    "    ['MinMax', 'Standard', 'Robust']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
